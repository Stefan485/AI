
using device: cuda
loaded: 6829285
1 epoch = 3334 batches
number of parameters: 26.48M
num decayed parameter tensors: 50, with 26,578,944 parameters
num non-decayed parameter tensors: 98, with 45,504 parameters
D:\Dev\Code\Playground\AI\gpt2\model_karpathy.py:64: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
using fused AdamW: True
step: 0, loss: (10.858227729797363,) dt: 520.94ms
step: 1, loss: (10.715126991271973,) dt: 320.42ms
step: 2, loss: (10.48857593536377,) dt: 346.84ms
step: 3, loss: (10.256766319274902,) dt: 361.83ms
step: 4, loss: (10.084774017333984,) dt: 325.31ms
step: 5, loss: (9.901269912719727,) dt: 314.61ms
step: 6, loss: (9.740618705749512,) dt: 314.87ms
step: 7, loss: (9.469157218933105,) dt: 334.92ms
step: 8, loss: (9.416216850280762,) dt: 328.31ms
step: 9, loss: (9.26562786102295,) dt: 324.95ms
step: 10, loss: (9.08176326751709,) dt: 323.86ms
step: 11, loss: (8.699511528015137,) dt: 325.73ms
step: 12, loss: (8.653837203979492,) dt: 319.89ms
step: 13, loss: (8.344501495361328,) dt: 317.58ms
step: 14, loss: (8.26445484161377,) dt: 320.52ms
step: 15, loss: (8.068568229675293,) dt: 317.06ms
step: 16, loss: (10.303943634033203,) dt: 328.98ms
step: 17, loss: (7.934911727905273,) dt: 326.99ms
step: 18, loss: (7.613229274749756,) dt: 313.92ms
step: 19, loss: (7.489118576049805,) dt: 297.78ms
step: 20, loss: (7.467552185058594,) dt: 290.41ms
step: 21, loss: (7.310770034790039,) dt: 301.28ms
step: 22, loss: (7.299066066741943,) dt: 307.34ms
step: 23, loss: (7.134758472442627,) dt: 290.93ms
step: 24, loss: (7.164622783660889,) dt: 310.40ms
step: 25, loss: (6.984460353851318,) dt: 311.73ms
step: 26, loss: (6.872892379760742,) dt: 295.29ms
step: 27, loss: (6.95219087600708,) dt: 305.68ms
step: 28, loss: (6.855953216552734,) dt: 308.99ms
step: 29, loss: (6.800233840942383,) dt: 297.85ms
step: 30, loss: (6.7695817947387695,) dt: 326.96ms
step: 31, loss: (6.7571821212768555,) dt: 306.09ms
step: 32, loss: (6.750481128692627,) dt: 295.33ms
step: 33, loss: (6.850005149841309,) dt: 300.32ms
step: 34, loss: (6.8180084228515625,) dt: 292.80ms
step: 35, loss: (6.560189247131348,) dt: 312.84ms
step: 36, loss: (6.549083709716797,) dt: 303.47ms
step: 37, loss: (6.568830966949463,) dt: 291.34ms
step: 38, loss: (6.682260036468506,) dt: 307.51ms
step: 39, loss: (7.193295001983643,) dt: 314.36ms
step: 40, loss: (6.135308265686035,) dt: 304.19ms
step: 41, loss: (6.841997146606445,) dt: 305.10ms
step: 42, loss: (6.646504878997803,) dt: 310.76ms
step: 43, loss: (6.652926921844482,) dt: 304.18ms
step: 44, loss: (6.529659271240234,) dt: 302.32ms
step: 45, loss: (6.4096527099609375,) dt: 311.59ms
step: 46, loss: (6.400749206542969,) dt: 312.70ms
step: 47, loss: (6.591930866241455,) dt: 314.14ms
step: 48, loss: (6.389091491699219,) dt: 311.02ms
step: 49, loss: (6.570992469787598,) dt: 321.24ms
>  Hello I'm a language model, for its&# be for scam= and is= of like  " the and a be.  " a   a& This you can, is scam,mer I   " would is scam
>  Hello I'm a language model, is. , scam> that for> so?  that of them with AI its the,= the.mer you forbr in is to this&qu; sc; of an in. it>
>  Hello I'm a language model,>&  be. for do: the((" have&#39 sc and of. sc&otR and that, your AIqu; be AI   the you on can up thea that
>  Hello I'm a language model,& thea is can like this ot is with scam on the,br of this your! the so just: with. AI  "mer to could of AI> so&> on AI abr
>  Hello I'm a language model, to abr scam them, tos I to the is a is you>> your&#39ot;br=&qu it AI for=> on AI sc of the this of the the can>