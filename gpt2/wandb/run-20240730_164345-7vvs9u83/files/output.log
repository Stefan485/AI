
using device: cuda
loaded: 11909059
1 epoch = 5814 batches
number of parameters: 26.48M
num decayed parameter tensors: 50, with 26,578,944 parameters
num non-decayed parameter tensors: 98, with 45,504 parameters
using fused AdamW: True
step: 0, loss: (10.873543739318848,) dt: 517.16ms
D:\Dev\Code\Playground\AI\gpt2\model_karpathy.py:64: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
step: 1, loss: (10.479046821594238,) dt: 320.74ms
step: 2, loss: (10.211618423461914,) dt: 326.36ms
step: 3, loss: (10.113188743591309,) dt: 309.40ms
step: 4, loss: (9.784225463867188,) dt: 291.68ms
step: 5, loss: (9.521273612976074,) dt: 316.77ms
step: 6, loss: (9.438277244567871,) dt: 291.09ms
step: 7, loss: (9.37874984741211,) dt: 293.11ms
step: 8, loss: (8.994279861450195,) dt: 299.43ms
step: 9, loss: (12.668623924255371,) dt: 293.16ms
step: 10, loss: (8.71076774597168,) dt: 299.88ms
step: 11, loss: (8.336841583251953,) dt: 291.76ms
step: 12, loss: (8.33508014678955,) dt: 301.46ms
step: 13, loss: (8.219915390014648,) dt: 298.77ms
step: 14, loss: (7.889350891113281,) dt: 308.51ms
step: 15, loss: (7.899785995483398,) dt: 291.09ms
step: 16, loss: (7.644213676452637,) dt: 312.77ms
step: 17, loss: (7.596561431884766,) dt: 300.72ms
step: 18, loss: (7.1513800621032715,) dt: 299.71ms
step: 19, loss: (7.3816046714782715,) dt: 300.57ms
step: 20, loss: (7.171137809753418,) dt: 308.40ms
step: 21, loss: (6.922240257263184,) dt: 322.20ms
step: 22, loss: (6.851919174194336,) dt: 327.10ms
step: 23, loss: (6.80805778503418,) dt: 309.10ms
step: 24, loss: (6.769567966461182,) dt: 299.81ms
step: 25, loss: (6.871009349822998,) dt: 300.28ms
step: 26, loss: (6.526286602020264,) dt: 301.52ms
step: 27, loss: (6.60729455947876,) dt: 299.28ms
step: 28, loss: (6.379750728607178,) dt: 300.07ms
step: 29, loss: (6.52016544342041,) dt: 299.04ms
step: 30, loss: (6.296543121337891,) dt: 302.24ms
step: 31, loss: (6.334534168243408,) dt: 299.50ms
step: 32, loss: (6.475627899169922,) dt: 321.47ms
step: 33, loss: (6.1918439865112305,) dt: 308.00ms
step: 34, loss: (6.540700912475586,) dt: 331.11ms
step: 35, loss: (6.379326343536377,) dt: 319.10ms
step: 36, loss: (6.004471778869629,) dt: 303.62ms
step: 37, loss: (6.3302154541015625,) dt: 300.96ms
step: 38, loss: (6.638944149017334,) dt: 299.79ms
step: 39, loss: (6.490518093109131,) dt: 302.60ms
step: 40, loss: (6.149111270904541,) dt: 300.89ms
step: 41, loss: (6.404299736022949,) dt: 314.01ms
step: 42, loss: (6.372056007385254,) dt: 302.63ms
step: 43, loss: (6.1233625411987305,) dt: 302.25ms
step: 44, loss: (6.199538707733154,) dt: 299.82ms
step: 45, loss: (6.133729457855225,) dt: 301.45ms
step: 46, loss: (6.1127471923828125,) dt: 302.01ms
step: 47, loss: (6.121392250061035,) dt: 304.88ms
step: 48, loss: (6.41586446762085,) dt: 316.02ms
step: 49, loss: (6.22293758392334,) dt: 303.13ms
>  Hello I'm a language model, bitchga it it with be for myiggas know my n like,: And's a be geted: my niggas do n you was Lil they on,) I niggas I don't
>  Hello I'm a language model,Share got a, we was me in he'mga like, we ain't wasga get theiggas get that they get for you don't to it me (�'s my I'm in's it,
>  Hello I'm a language model,> just < be, the n: the Lil I don't ain't niggas me I like you know,Ch that,Ch she get bitch, I no'm my ain't can ( the� that
>  Hello I'm a language model, shit the wasas they like your's it (' Lil that the never shit of me your'm the nas: withga me's bitch, on thatse 2 it on get a on ' a don't
>  Hello I'm a language model, myOS bitch with my, you don't know the shit a's you no don't be don't fromin' shit, they'm myiggas don't� in)] we] don't can't