
using device: cuda
loaded: 11909059
1 epoch = 5814 batches
number of parameters: 26.48M
num decayed parameter tensors: 50, with 26,578,944 parameters
num non-decayed parameter tensors: 98, with 45,504 parameters
using fused AdamW: True
step: 0, loss: (10.873543739318848,) dt: 915.61ms
D:\Dev\Code\Playground\AI\gpt2\model_karpathy.py:64: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
step: 1, loss: (10.479046821594238,) dt: 299.54ms
step: 2, loss: (10.211618423461914,) dt: 309.08ms
step: 3, loss: (10.113186836242676,) dt: 300.71ms
step: 4, loss: (9.784225463867188,) dt: 306.11ms
step: 5, loss: (9.521273612976074,) dt: 314.10ms
step: 6, loss: (9.438277244567871,) dt: 304.36ms
step: 7, loss: (9.37874984741211,) dt: 312.51ms
step: 8, loss: (8.994279861450195,) dt: 312.38ms
step: 9, loss: (12.668622970581055,) dt: 303.85ms
step: 10, loss: (8.710763931274414,) dt: 317.47ms
step: 11, loss: (8.33683967590332,) dt: 308.05ms
step: 12, loss: (8.335077285766602,) dt: 318.13ms
step: 13, loss: (8.219914436340332,) dt: 313.83ms
step: 14, loss: (7.889348030090332,) dt: 301.37ms
step: 15, loss: (7.899782180786133,) dt: 307.67ms
step: 16, loss: (7.644210338592529,) dt: 316.06ms
step: 17, loss: (7.596558094024658,) dt: 299.50ms
step: 18, loss: (7.151378631591797,) dt: 307.59ms
step: 19, loss: (7.3816022872924805,) dt: 311.44ms
step: 20, loss: (7.171133995056152,) dt: 308.01ms
step: 21, loss: (6.922229766845703,) dt: 286.25ms
step: 22, loss: (6.851917266845703,) dt: 286.07ms
step: 23, loss: (6.808136463165283,) dt: 301.44ms
step: 24, loss: (6.768100738525391,) dt: 325.60ms
step: 25, loss: (6.869772434234619,) dt: 305.02ms
step: 26, loss: (6.523433685302734,) dt: 299.96ms
step: 27, loss: (6.605027198791504,) dt: 303.86ms
step: 28, loss: (6.374218940734863,) dt: 316.30ms
step: 29, loss: (6.52179479598999,) dt: 364.68ms
step: 30, loss: (6.299895286560059,) dt: 333.01ms
step: 31, loss: (6.334479331970215,) dt: 323.27ms
step: 32, loss: (6.474194049835205,) dt: 306.20ms
step: 33, loss: (6.186091899871826,) dt: 350.01ms
step: 34, loss: (6.537223815917969,) dt: 338.78ms
step: 35, loss: (6.375506401062012,) dt: 321.17ms
step: 36, loss: (6.004762172698975,) dt: 318.64ms
step: 37, loss: (6.327127456665039,) dt: 360.09ms
step: 38, loss: (6.637230396270752,) dt: 350.49ms
step: 39, loss: (6.48167610168457,) dt: 325.14ms
step: 40, loss: (6.131678581237793,) dt: 330.79ms
step: 41, loss: (6.3865814208984375,) dt: 323.09ms
step: 42, loss: (6.359045505523682,) dt: 330.65ms
step: 43, loss: (6.11243200302124,) dt: 324.40ms
step: 44, loss: (6.20530891418457,) dt: 325.03ms
step: 45, loss: (6.114424228668213,) dt: 327.55ms
step: 46, loss: (6.076346397399902,) dt: 325.77ms
step: 47, loss: (6.085580348968506,) dt: 309.54ms
step: 48, loss: (6.4167704582214355,) dt: 303.56ms
step: 49, loss: (6.213672161102295,) dt: 300.75ms
>  Hello I'm a language model,
>  Hello I'm a language model,
>  Hello I'm a language model,
>  Hello I'm a language model,
>  Hello I'm a language model,